Impl.java:0, took 169.622027 ms
root
 |-- order_id: integer (nullable = true)
 |-- order_date: date (nullable = true)
 |-- customer_name: string (nullable = true)
 |-- quantity: integer (nullable = true)
 |-- unit_price: double (nullable = true)

Writing to:  s3a://my-first-s3-data-lake/WWI/
Traceback (most recent call last):
  File "/home/wgeesey/airflow/dags/spark_jobs/transform_orders.py", line 65, in <module>
    main()
  File "/home/wgeesey/airflow/dags/spark_jobs/transform_orders.py", line 59, in main
    .parquet(args.output_path)
     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 2003, in parquet
  File "/opt/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 282, in deco
  File "/opt/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o63.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource.makeQualified(DataSource.scala:125)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:468)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:554)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	... 26 more

26/02/12 13:43:22 INFO SparkContext: Invoking stop() from shutdown hook
26/02/12 13:43:22 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:572.
26/02/12 13:43:22 INFO SparkUI: Stopped Spark web UI at http://DataEngineering:4040
26/02/12 13:43:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/12 13:43:22 INFO MemoryStore: MemoryStore cleared
26/02/12 13:43:22 INFO BlockManager: BlockManager stopped
26/02/12 13:43:22 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/12 13:43:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/12 13:43:22 INFO SparkContext: Successfully stopped SparkContext
26/02/12 13:43:22 INFO ShutdownHookManager: Shutdown hook called
26/02/12 13:43:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-d613e855-97e7-4500-adfe-ad1ec7c93c46
26/02/12 13:43:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-ded87eb6-d4ed-413a-b3a2-a6b14afc455f
26/02/12 13:43:22 INFO ShutdownHookManager: Deleting directory /tmp/artifacts-3c434c84-73ba-4075-beec-921d41d99738
26/02/12 13:43:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-d613e855-97e7-4500-adfe-ad1ec7c93c46/pyspark-5e97ea53-f221-4e6f-9f47-745361cddef1

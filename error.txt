DataEngineering
 ▶ Log message source details
[2026-02-16, 17:06:56 UTC] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2026-02-16, 17:06:56 UTC] {base.py:84} INFO - Retrieving connection 'spark_default'
[2026-02-16, 17:06:56 UTC] {spark_submit.py:473} INFO - Spark-Submit cmd: spark-submit --master  --name arrow-spark ~/airflow/dags/spark_jobs/transform_orders.py --input_path ~/airflow/data/staging/orders.csv --output_path s3a://my-first-s3-data-lake/WWI/
[2026-02-16, 17:06:58 UTC] {spark_submit.py:634} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, k8s, or local
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:1045)
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:264)
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)
[2026-02-16, 17:07:00 UTC] {spark_submit.py:634} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2026-02-16, 17:07:00 UTC] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 422, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 176, in execute
    self._hook.submit(self.application)
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 560, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --name arrow-spark ~/airflow/dags/spark_jobs/transform_orders.py --input_path ~/airflow/data/staging/orders.csv --output_path s3a://my-first-s3-data-lake/WWI/. Error code is: 1.
[2026-02-16, 17:07:00 UTC] {logging_mixin.py:190} INFO - Task instance in failure state
[2026-02-16, 17:07:00 UTC] {logging_mixin.py:190} INFO - Task start:2026-02-16 17:06:56.763480+00:00 end:2026-02-16 17:07:00.503822+00:00 duration:3.740342
[2026-02-16, 17:07:00 UTC] {logging_mixin.py:190} INFO - Task:<Task(SparkSubmitOperator): spark_transform_orders> dag:<DAG: mssql_extract_test> dagrun:<DagRun mssql_extract_test @ 2026-02-09 20:01:46+00:00: manual__2026-02-09T20:01:46+00:00, state:running, queued_at: 2026-02-16 17:06:50.919691+00:00. externally triggered: True>
[2026-02-16, 17:07:00 UTC] {logging_mixin.py:190} INFO - Failure caused by Cannot execute: spark-submit --master  --name arrow-spark ~/airflow/dags/spark_jobs/transform_orders.py --input_path ~/airflow/data/staging/orders.csv --output_path s3a://my-first-s3-data-lake/WWI/. Error code is: 1.
[2026-02-16, 17:07:00 UTC] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=mssql_extract_test, task_id=spark_transform_orders, run_id=manual__2026-02-09T20:01:46+00:00, execution_date=20260209T200146, start_date=20260216T170656, end_date=20260216T170700
[2026-02-16, 17:07:00 UTC] {taskinstance.py:340} ▶ Post task execution logs

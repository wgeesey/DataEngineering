els
DataEngineering
 ▶ Log message source details
[2026-02-16, 20:07:09 UTC] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2026-02-16, 20:07:09 UTC] {base.py:84} INFO - Retrieving connection 'spark_default'
[2026-02-16, 20:07:09 UTC] {spark_submit.py:473} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark --deploy-mode client /home/wgeesey/airflow/dags/spark_jobs/transform_orders.py --input_path /home/wgeesey/airflow/data/staging/orders.csv --output_path s3a://my-first-s3-data-lake/WWI/
[2026-02-16, 20:07:10 UTC] {spark_submit.py:634} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO SparkContext: Running Spark version 4.0.1
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO SparkContext: OS info Linux, 6.17.0-14-generic, amd64
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO SparkContext: Java version 21.0.10
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO ResourceUtils: ==============================================================
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO ResourceUtils: No custom resources configured for spark.driver.
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO ResourceUtils: ==============================================================
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO SparkContext: Submitted application: WideWorldImportersTransform
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO ResourceProfile: Limiting resource is cpu
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO SecurityManager: Changing view acls to: wgeesey
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO SecurityManager: Changing modify acls to: wgeesey
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO SecurityManager: Changing view acls groups to: wgeesey
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO SecurityManager: Changing modify acls groups to: wgeesey
[2026-02-16, 20:07:14 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: wgeesey groups with view permissions: EMPTY; users with modify permissions: wgeesey; groups with modify permissions: EMPTY; RPC SSL disabled
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO Utils: Successfully started service 'sparkDriver' on port 44211.
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO SparkEnv: Registering MapOutputTracker
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO SparkEnv: Registering BlockManagerMaster
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bce6f637-ed92-4cad-ac9c-34449e62f1fb
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO SparkEnv: Registering OutputCommitCoordinator
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO SecurityManager: Changing view acls to: wgeesey
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO SecurityManager: Changing modify acls to: wgeesey
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO SecurityManager: Changing view acls groups to: wgeesey
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO SecurityManager: Changing modify acls groups to: wgeesey
[2026-02-16, 20:07:15 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: wgeesey groups with view permissions: EMPTY; users with modify permissions: wgeesey; groups with modify permissions: EMPTY; RPC SSL disabled
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO Executor: Starting executor ID driver on host DataEngineering
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO Executor: OS info Linux, 6.17.0-14-generic, amd64
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO Executor: Java version 21.0.10
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@85fe3cc for default.
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38711.
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO NettyBlockTransferService: Server created on DataEngineering:38711
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DataEngineering, 38711, None)
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO BlockManagerMasterEndpoint: Registering block manager DataEngineering:38711 with 1048.8 MiB RAM, BlockManagerId(driver, DataEngineering, 38711, None)
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DataEngineering, 38711, None)
[2026-02-16, 20:07:16 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DataEngineering, 38711, None)
[2026-02-16, 20:07:20 UTC] {spark_submit.py:634} INFO - Reading CSV from: /home/wgeesey/airflow/data/staging/orders.csv
[2026-02-16, 20:07:20 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2026-02-16, 20:07:20 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:20 INFO SharedState: Warehouse path is 'file:/home/wgeesey/spark-warehouse'.
[2026-02-16, 20:07:21 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:21 INFO InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.
[2026-02-16, 20:07:21 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:21 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2026-02-16, 20:07:22 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:22 INFO FileSourceStrategy: Pushed Filters:
[2026-02-16, 20:07:22 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:22 INFO FileSourceStrategy: Post-Scan Filters: Set((length(trim(value#0, None)) > 0))
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO CodeGenerator: Code generated in 357.758097 ms
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 224.6 KiB, free 1048.6 MiB)
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1048.5 MiB)
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO DAGScheduler: Parents of final stage: List()
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO DAGScheduler: Missing parents: List()
[2026-02-16, 20:07:23 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.3 KiB, free 1048.5 MiB)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 1048.5 MiB)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1676
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DataEngineering,executor driver, partition 0, PROCESS_LOCAL, 10246 bytes)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO CodeGenerator: Code generated in 26.389558 ms
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO FileScanRDD: Reading File path: file:///home/wgeesey/airflow/data/staging/orders.csv, range: 0-2467, partition values: [empty row]
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO CodeGenerator: Code generated in 27.733639 ms
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1579 bytes result sent to driver
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 454 ms on DataEngineering (executor driver) (1/1)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 630 ms
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO TaskSchedulerImpl: Canceling stage 0
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 710.997475 ms
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO CodeGenerator: Code generated in 10.591419 ms
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO FileSourceStrategy: Pushed Filters:
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 224.6 KiB, free 1048.3 MiB)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1048.3 MiB)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: Parents of final stage: List()
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: Missing parents: List()
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 23.2 KiB, free 1048.5 MiB)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.6 KiB, free 1048.5 MiB)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1676
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DataEngineering,executor driver, partition 0, PROCESS_LOCAL, 10246 bytes)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO CodeGenerator: Code generated in 14.913156 ms
[2026-02-16, 20:07:24 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:24 INFO FileScanRDD: Reading File path: file:///home/wgeesey/airflow/data/staging/orders.csv, range: 0-2467, partition values: [empty row]
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1547 bytes result sent to driver
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 122 ms on DataEngineering (executor driver) (1/1)
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 162 ms
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO TaskSchedulerImpl: Canceling stage 1
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 167.254949 ms
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - root
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - |-- order_id: integer (nullable = true)
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - |-- order_date: date (nullable = true)
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - |-- customer_name: string (nullable = true)
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - |-- quantity: integer (nullable = true)
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - |-- unit_price: double (nullable = true)
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - Writing to:  s3a://my-first-s3-data-lake/WWI/
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 WARN FileSystem: Failed to initialize filesystem s3a://my-first-s3-data-lake/WWI: java.lang.NumberFormatException: For input string: "60s"
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - Traceback (most recent call last):
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - File "/home/wgeesey/airflow/dags/spark_jobs/transform_orders.py", line 71, in <module>
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - main()
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - File "/home/wgeesey/airflow/dags/spark_jobs/transform_orders.py", line 59, in main
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - .parquet(args.output_path)
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 2003, in parquet
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - File "/opt/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 288, in deco
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - pyspark.errors.exceptions.captured.NumberFormatException: For input string: "60s"
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO SparkContext: Invoking stop() from shutdown hook
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:572.
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO SparkUI: Stopped Spark web UI at http://DataEngineering:4040
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO MemoryStore: MemoryStore cleared
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO BlockManager: BlockManager stopped
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO SparkContext: Successfully stopped SparkContext
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO ShutdownHookManager: Shutdown hook called
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-57ff2dc9-f6e8-4d8a-92ea-25c62422b7d3
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0b1d0ba-6d28-4a34-8286-f85ce453a81a/pyspark-80d30d88-e3ce-44de-8320-713c259c9b64
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO ShutdownHookManager: Deleting directory /tmp/artifacts-58570802-3b1d-4f7b-82bd-6971f0c8a6bd
[2026-02-16, 20:07:25 UTC] {spark_submit.py:634} INFO - 26/02/16 20:07:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0b1d0ba-6d28-4a34-8286-f85ce453a81a
[2026-02-16, 20:07:25 UTC] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 422, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 176, in execute
    self._hook.submit(self.application)
  File "/home/wgeesey/airflow/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 560, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local[*] --name arrow-spark --deploy-mode client /home/wgeesey/airflow/dags/spark_jobs/transform_orders.py --input_path /home/wgeesey/airflow/data/staging/orders.csv --output_path s3a://my-first-s3-data-lake/WWI/. Error code is: 1.
[2026-02-16, 20:07:25 UTC] {logging_mixin.py:190} INFO - Task instance in failure state
[2026-02-16, 20:07:25 UTC] {logging_mixin.py:190} INFO - Task start:2026-02-16 20:07:09.585136+00:00 end:2026-02-16 20:07:25.754395+00:00 duration:16.169259
[2026-02-16, 20:07:25 UTC] {logging_mixin.py:190} INFO - Task:<Task(SparkSubmitOperator): spark_transform_orders> dag:<DAG: mssql_extract_test> dagrun:<DagRun mssql_extract_test @ 2026-02-09 20:01:46+00:00: manual__2026-02-09T20:01:46+00:00, state:running, queued_at: 2026-02-16 20:07:07.095235+00:00. externally triggered: True>
[2026-02-16, 20:07:25 UTC] {logging_mixin.py:190} INFO - Failure caused by Cannot execute: spark-submit --master local[*] --name arrow-spark --deploy-mode client /home/wgeesey/airflow/dags/spark_jobs/transform_orders.py --input_path /home/wgeesey/airflow/data/staging/orders.csv --output_path s3a://my-first-s3-data-lake/WWI/. Error code is: 1.
[2026-02-16, 20:07:25 UTC] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=mssql_extract_test, task_id=spark_transform_orders, run_id=manual__2026-02-09T20:01:46+00:00, execution_date=20260209T200146, start_date=20260216T200709, end_date=20260216T200725
[2026-02-16, 20:07:25 UTC] {taskinstance.py:340} ▶ Post task execution logs
